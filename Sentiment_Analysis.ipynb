{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CSCI544: Homework Assignment No2\n",
        "##### Due on February 8, 2024 (before class)\n",
        "##### Name - Hrishikesh Thakur\n",
        "##### Student ID - 5980681484\n",
        "\n",
        "Note - Answers to questions are present at the bottom of .ipynb under the results and observation tab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9JRnHqVdenP",
        "outputId": "4cea6877-005b-49d3-e341-4d49c1e6987f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     /Users/hrishikesh/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/hrishikesh/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /Users/hrishikesh/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/hrishikesh/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import defaultdict\n",
        "\n",
        "from gensim import utils\n",
        "from gensim.test.utils import datapath\n",
        "from gensim.models import Word2Vec\n",
        "import gensim.downloader as api\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk import word_tokenize, pos_tag\n",
        "\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ! pip install bs4 # in case you don't have it installed\n",
        "# ! pip install gensim\n",
        "# ! pip install torch\n",
        "# ! pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBC96GEddenR"
      },
      "source": [
        "## Read Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eheZHyzTdenS",
        "outputId": "67ed84fe-2474-4ef4-9499-3b93babd2c2e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/jc/l_cy7_tj5pn7bp93mtmcrhg80000gn/T/ipykernel_5994/971836897.py:1: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  dataframe = pd.read_csv(\"https://web.archive.org/web/20201127142707if_/https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Office_Products_v1_00.tsv.gz\", sep=\"\\t\", on_bad_lines=\"skip\")\n"
          ]
        }
      ],
      "source": [
        "dataframe = pd.read_csv(\"https://web.archive.org/web/20201127142707if_/https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Office_Products_v1_00.tsv.gz\", sep=\"\\t\", on_bad_lines=\"skip\")\n",
        "# dataframe.to_csv('dataframe.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Dataset Generation\n",
        "### Keep Ratings, Review headline and Review body \n",
        "- The code selects specific columns (\"star_rating\", \"review_headline\", \"review_body\") from the original dataframe, presumably containing review data.\n",
        "\n",
        "- It renames the \"star_rating\" column to \"rating\" for clarity and consistency.\n",
        "\n",
        "- The code concatenates the \"review_headline\" and \"review_body\" columns to form a single \"review\" column, which contains the complete text of each review.\n",
        "\n",
        "- It converts the \"rating\" column to numeric format, handling any conversion errors gracefully by coercing errors. This ensures that only valid numeric ratings are retained.\n",
        "\n",
        "- Any rows with missing values (NaNs) in the dataframe are dropped using dropna() function to ensure data cleanliness and consistency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jExD9OUFdenT",
        "outputId": "f6562afe-0f93-4915-d492-70ae733854d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- Three sample reviews before data cleaning + preprocessing.\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rating</th>\n",
              "      <th>review_headline</th>\n",
              "      <th>review_body</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.0</td>\n",
              "      <td>Five Stars</td>\n",
              "      <td>Great product.</td>\n",
              "      <td>Five Stars Great product.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5.0</td>\n",
              "      <td>Phffffffft, Phfffffft. Lots of air, and it's C...</td>\n",
              "      <td>What's to say about this commodity item except...</td>\n",
              "      <td>Phffffffft, Phfffffft. Lots of air, and it's C...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5.0</td>\n",
              "      <td>but I am sure I will like it.</td>\n",
              "      <td>Haven't used yet, but I am sure I will like it.</td>\n",
              "      <td>but I am sure I will like it. Haven't used yet...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   rating                                    review_headline  \\\n",
              "0     5.0                                         Five Stars   \n",
              "1     5.0  Phffffffft, Phfffffft. Lots of air, and it's C...   \n",
              "2     5.0                      but I am sure I will like it.   \n",
              "\n",
              "                                         review_body  \\\n",
              "0                                     Great product.   \n",
              "1  What's to say about this commodity item except...   \n",
              "2    Haven't used yet, but I am sure I will like it.   \n",
              "\n",
              "                                              review  \n",
              "0                          Five Stars Great product.  \n",
              "1  Phffffffft, Phfffffft. Lots of air, and it's C...  \n",
              "2  but I am sure I will like it. Haven't used yet...  "
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Keep Reviews and Ratings\n",
        "# dataframe = pd.read_csv('./dataframe.csv')\n",
        "df = dataframe[[\"star_rating\", \"review_headline\", \"review_body\"]].copy()  # Make a copy to avoid modifying the original DataFrame\n",
        "df.rename(columns={\"star_rating\": \"rating\"}, inplace=True)\n",
        "df[\"review\"] = df.review_headline + ' ' + df.review_body\n",
        "df[\"rating\"] = pd.to_numeric(df.rating, errors=\"coerce\")\n",
        "df = df.dropna()\n",
        "print(\"- Three sample reviews before data cleaning + preprocessing.\\n\")\n",
        "df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below cell computes the frequency distribution of ratings in a DataFrame (df) and stores it in rating_stats. Then, it iterates over the items in rating_stats, printing the number of reviews for each unique rating value along with its frequency. This essentially provides a summary of how many reviews exist for each rating value present in the DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of reviews having rating 5.0 : 1582682\n",
            "Number of reviews having rating 4.0 : 418339\n",
            "Number of reviews having rating 1.0 : 306962\n",
            "Number of reviews having rating 3.0 : 193674\n",
            "Number of reviews having rating 2.0 : 138380\n"
          ]
        }
      ],
      "source": [
        "rating_stats = df.rating.value_counts()\n",
        "for i , j in rating_stats.items():\n",
        "    print(f'Number of reviews having rating {i} : {j}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  We form three classes and select 50000 reviews randomly for every rating.\n",
        "\n",
        "- The code snippet uses a lambda function to encode the ratings into three categories: 1 for ratings greater than 3, 2 for ratings less than 3, and 3 for ratings equal to 3. This creates a categorical variable that simplifies the sentiment analysis task by converting the ratings into positive, negative, and neutral sentiment categories.\n",
        "\n",
        "- The ratings are then sampled to create a balanced dataset for training the sentiment analysis model. Each rating category is sampled to have 50,000 samples using the sample method with a specified random seed (random_state=42). This ensures that each sentiment category has an equal representation in the dataset, which is important for training a robust sentiment analysis model.\n",
        "\n",
        "- After sampling, the dataframes corresponding to each rating category are concatenated using pd.concat. This combines the sampled dataframes into a single dataframe called dataset, which will be used for training the sentiment analysis model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rating</th>\n",
              "      <th>review_headline</th>\n",
              "      <th>review_body</th>\n",
              "      <th>review</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>not  worse of money</td>\n",
              "      <td>The keyboard is not sensitive enough and it ta...</td>\n",
              "      <td>not  worse of money The keyboard is not sensit...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>Price?</td>\n",
              "      <td>How come you can buy this on Sony's site for $...</td>\n",
              "      <td>Price? How come you can buy this on Sony's sit...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>Black is light grey @ 60% full status</td>\n",
              "      <td>Well I was happy to save a lot of money *at fi...</td>\n",
              "      <td>Black is light grey @ 60% full status Well I w...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>I am not please with my order. i replaced ...</td>\n",
              "      <td>I am not please with my order. i replaced the ...</td>\n",
              "      <td>I am not please with my order. i replaced ... ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>Does not work.</td>\n",
              "      <td>Bought a new (not refurbished) one. When I pla...</td>\n",
              "      <td>Does not work. Bought a new (not refurbished) ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   rating                                review_headline  \\\n",
              "0     1.0                            not  worse of money   \n",
              "1     1.0                                         Price?   \n",
              "2     1.0          Black is light grey @ 60% full status   \n",
              "3     1.0  I am not please with my order. i replaced ...   \n",
              "4     1.0                                 Does not work.   \n",
              "\n",
              "                                         review_body  \\\n",
              "0  The keyboard is not sensitive enough and it ta...   \n",
              "1  How come you can buy this on Sony's site for $...   \n",
              "2  Well I was happy to save a lot of money *at fi...   \n",
              "3  I am not please with my order. i replaced the ...   \n",
              "4  Bought a new (not refurbished) one. When I pla...   \n",
              "\n",
              "                                              review  labels  \n",
              "0  not  worse of money The keyboard is not sensit...       2  \n",
              "1  Price? How come you can buy this on Sony's sit...       2  \n",
              "2  Black is light grey @ 60% full status Well I w...       2  \n",
              "3  I am not please with my order. i replaced ... ...       2  \n",
              "4  Does not work. Bought a new (not refurbished) ...       2  "
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df[\"labels\"] = df.rating.apply(lambda x: 1 if x > 3 else (2 if x < 3 else 3))\n",
        "ratings = [df[df[\"rating\"] == i].sample(50000,random_state=42) for i in range(1, 6)]\n",
        "dataset = pd.concat(ratings, ignore_index=True)\n",
        "# filtered_dataset.to_csv('filtered_dataset.csv')\n",
        "# filtered_dataset.head(20)\n",
        "# dataset = pd.read_csv('./filtered_dataset.csv')\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Custom function for Lemmatization\n",
        "\n",
        "- The lemmatization process involves converting words to their base form to normalize variations of the same word. For example, \"running\" would be converted to \"run\", \"better\" to \"good\", etc. This helps in reducing the dimensionality of the vocabulary and improving the performance of natural language processing tasks.\n",
        "\n",
        "- Part-of-Speech (POS) Tagging: In order to perform accurate lemmatization, it is important to provide the lemmatizer with the correct part-of-speech (POS) tag for each word. This is because a word may have different lemmas depending on its grammatical role in a sentence (e.g., verb, noun, adjective, adverb).\n",
        "\n",
        "- POS Tagging with NLTK: The code snippet uses NLTK's pos_tag function to perform POS tagging on the input text. This function assigns a POS tag to each word in the sentence, indicating its grammatical category (e.g., noun, verb, adjective, adverb).\n",
        "\n",
        "- Mapping POS Tags to WordNet POS Tags: The POS tags returned by NLTK are mapped to corresponding WordNet POS tags using a dictionary (tag_map). This mapping ensures that the lemmatizer understands the grammatical context of each word and selects the appropriate lemma.\n",
        "\n",
        "- Lemmatization Process: Finally, the lemmatizer applies lemmatization to each word in the input text based on its POS tag. It returns a string containing the lemmatized version of the input text, where each word has been replaced with its base form."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "# WordNet Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_word_according_pos(statement):\n",
        "    tag_map = defaultdict(lambda: wn.NOUN)\n",
        "    tag_map['J'] = wn.ADJ\n",
        "    tag_map['V'] = wn.VERB\n",
        "    tag_map['R'] = wn.ADV\n",
        "    tokens = word_tokenize(statement)\n",
        "    return ' '.join([lemmatizer.lemmatize(token, tag_map[tag[0]]) for token, tag in pos_tag(tokens)])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Cleaning & Preprecessing\n",
        "\n",
        "- data_cleaning and data_preprocessing collectively performs following task: \n",
        "\n",
        "    - The function converts the text to lowercase and removes any URLs or HTML tags present in the text using regular expressions.\n",
        "\n",
        "    - BeautifulSoup Parsing: BeautifulSoup library is used to parse the HTML content and extract the text, effectively removing any HTML tags.\n",
        "\n",
        "    - Special Character Removal: Special characters, such as punctuation marks and symbols, are removed from the text using regular expressions.\n",
        "\n",
        "    - Handling Contractions: A dictionary contraction_dict is defined to map common contractions to their expanded forms. The function replaces contractions with their expanded forms, ensuring uniformity in the text.\n",
        "\n",
        "    - Custom Stopword List: We have created a custom list of stopwords by excluding certain negation words ('no', 'nor', 'not', etc.) from the standard English stopwords list. This helps in preserving the negation context in the text, which is important for sentiment analysis.\n",
        "\n",
        "    - Stopword Removal: Stopwords, which are common words that do not carry significant meaning in the context of sentiment analysis, are removed from the text. This step helps in reducing noise and focusing on important keywords.\n",
        "\n",
        "    - Return Cleaned Text: The function returns the cleaned text after applying all the specified preprocessing steps. This clean text can then be used for further analysis or modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "# data cleaning function\n",
        "def data_cleaning (statement, stop_words):\n",
        "    contraction_dict = {\n",
        "        \"i'm\": \"i am\",\n",
        "        \"you're\": \"you are\",\n",
        "        \"he's\": \"he is\",\n",
        "        \"she's\": \"she is\",\n",
        "        \"they're\": \"they are\",\n",
        "        \"we're\": \"we are\",\n",
        "        \"it's\": \"it is\",\n",
        "        \"that's\": \"that is\",\n",
        "        \"here's\": \"here is\",\n",
        "        \"there's\": \"there is\",\n",
        "        \"who's\": \"who is\",\n",
        "        \"where's\": \"where is\",\n",
        "        \"when's\": \"when is\",\n",
        "        \"why's\": \"why is\",\n",
        "        \"what's\": \"what is\",\n",
        "        \"how's\": \"how is\",\n",
        "        \"everybody's\": \"everybody is\",\n",
        "        \"nobody's\": \"nobody is\",\n",
        "        \"something's\": \"something is\",\n",
        "        \"so's\": \"so is\",\n",
        "        \"i'll\": \"i will\",\n",
        "        \"you'll\": \"you will\",\n",
        "        \"he'll\": \"he will\",\n",
        "        \"she'll\": \"she will\",\n",
        "        \"they'll\": \"they will\",\n",
        "        \"it'll\": \"it will\",\n",
        "        \"we'll\": \"we will\",\n",
        "        \"that'll\": \"that will\",\n",
        "        \"this'll\": \"this will\",\n",
        "        \"these'll\": \"these will\",\n",
        "        \"there'll\": \"there will\",\n",
        "        \"where'll\": \"where will\",\n",
        "        \"who'll\": \"who will\",\n",
        "        \"what'll\": \"what will\",\n",
        "        \"how'll\": \"how will\",\n",
        "        \"i've\": \"i have\",\n",
        "        \"you've\": \"you have\",\n",
        "        \"he's\": \"he has\",\n",
        "        \"she's\": \"she has\",\n",
        "        \"we've\": \"we have\",\n",
        "        \"they've\": \"they have\",\n",
        "        \"should've\": \"should have\",\n",
        "        \"could've\": \"could have\",\n",
        "        \"would've\": \"would have\",\n",
        "        \"might've\": \"might have\",\n",
        "        \"must've\": \"must have\",\n",
        "        \"what've\": \"what have\",\n",
        "        \"what's\": \"what has\",\n",
        "        \"where've\": \"where have\",\n",
        "        \"where's\": \"where has\",\n",
        "        \"there've\": \"there have\",\n",
        "        \"there's\": \"there has\",\n",
        "        \"these've\": \"these have\",\n",
        "        \"who's\": \"who has\",\n",
        "        \"don't\": \"do not\",\n",
        "        \"can't\": \"cannot\",\n",
        "        \"mustn't\": \"must not\",\n",
        "        \"aren't\": \"are not\",\n",
        "        \"couldn't\": \"could not\",\n",
        "        \"wouldn't\": \"would not\",\n",
        "        \"shouldn't\": \"should not\",\n",
        "        \"isn't\": \"is not\",\n",
        "        \"doesn't\": \"does not\",\n",
        "        \"didn't\": \"did not\",\n",
        "        \"hasn't\": \"has not\",\n",
        "        \"hadn't\": \"had not\",\n",
        "        \"haven't\": \"have not\",\n",
        "        \"wasn't\": \"was not\",\n",
        "        \"won't\": \"will not\",\n",
        "        \"weren't\": \"were not\",\n",
        "        \"ain't\": \"am not\",\n",
        "        \"let's\": \"let us\",\n",
        "        \"y'all\": \"you all\",\n",
        "        \"where'd\": \"where did\",\n",
        "        \"how'd\": \"how did\",\n",
        "        \"why'd\": \"why did\",\n",
        "        \"who'd\": \"who did\",\n",
        "        \"when'd\": \"when did\",\n",
        "        \"what'd\": \"what did\",\n",
        "        \"g'day\": \"good day\",\n",
        "        \"ma'am\": \"madam\",\n",
        "        \"o'clock\": \"of the clock\"\n",
        "    }\n",
        "    # Lowercase and remove URLs and html tags\n",
        "    statement = re.sub(r'https?://\\S+|www\\.\\S+', ' ', statement.lower().strip())\n",
        "    # statement = re.sub(r'<[^<>]*>', ' ', statement)\n",
        "    soup = BeautifulSoup(statement, 'html.parser')\n",
        "    statement = soup.get_text(separator=' ', strip=True)\n",
        "    \n",
        "    # Remove special characters\n",
        "    statement = re.sub(r'[^a-zA-Z]', ' ', statement)\n",
        "    \n",
        "    # Handle contractions\n",
        "    statement = \" \".join([contraction_dict[word] if word in contraction_dict else word for word in statement.split()])\n",
        "\n",
        "    # Remove stopwords\n",
        "    statement = ' '.join([word for word in statement.split() if word not in stop_words])\n",
        "    \n",
        "    return statement\n",
        "\n",
        "# Data preprocessing function\n",
        "def data_preprocessing(data):\n",
        "    stop_words = stopwords.words('english')\n",
        "    stop_words = list(set(stop_words) - set(['no', 'nor','not', 'only', 'very', \"don't\", \"ain't\", \"aren't\", \"couldn't\", \"didn't\", \"doesn't\", \"hadn't\", \"hasn't\", \"might't\",\"musn't\", \"isn't\", \"needn't\", \"shan't\", \"shouldn't\", \"wasn't\", \"weren't\", \"wont't\", \"wouldn't\"]))\n",
        "    data[\"processed_review\"] = data.review.apply(lambda x: data_cleaning(x, stop_words))\n",
        "    data.processed_review = data.processed_review.apply(lambda x: lemmatize_word_according_pos(x))\n",
        "    return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Performing data cleaning and preprocessing "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHDG_y4JdenV",
        "outputId": "e7efe134-d39e-4d3f-ee2a-02bef062080b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/jc/l_cy7_tj5pn7bp93mtmcrhg80000gn/T/ipykernel_5994/194206676.py:90: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(statement, 'html.parser')\n"
          ]
        }
      ],
      "source": [
        "dataset = data_preprocessing(dataset)\n",
        "# dataset.to_csv('preprocessed_data.csv')\n",
        "# dataset = pd.read_csv('./preprocessed_data.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Word Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Pretrained Word2Vec Model:\n",
        "\n",
        "    - Utilizing api.load('word2vec-google-news-300') to load the pre-trained Word2Vec model from the gensim library.\n",
        "\n",
        "    - This specific model is the Google News Word2Vec model trained on a large corpus of Google News articles.\n",
        "\n",
        "- Sample Vocabulary:\n",
        "\n",
        "    - Iterating through the vocabulary of the loaded Word2Vec model to showcase some sample words. Printing the index and corresponding word for the first 5 entries in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "word #0/3000000 is </s>\n",
            "word #1/3000000 is in\n",
            "word #2/3000000 is for\n",
            "word #3/3000000 is that\n",
            "word #4/3000000 is is\n"
          ]
        }
      ],
      "source": [
        "# Pretrained Word2Vec Model\n",
        "pretrained_word2vec_model = api.load('word2vec-google-news-300')\n",
        "\n",
        "# sample list of vocabulary from word2vec-google-news-300\n",
        "for index, word in enumerate(pretrained_word2vec_model.index_to_key):\n",
        "    if index == 5:\n",
        "        break\n",
        "    print(f\"word #{index}/{len(pretrained_word2vec_model.index_to_key)} is {word}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pretrained semantic similarities of the generated vectors [('winter', 0.6970129609107971)]\n",
            "Pretrained semantic similarities of the generated vectors [('better', 0.735008955001831)]\n",
            "Pretrained semantic similarities of the generated vectors [('son', 0.8507932424545288)]\n"
          ]
        }
      ],
      "source": [
        "# printing semantic similarities of the generated vectors\n",
        "print(\"Pretrained semantic similarities of the generated vectors\", pretrained_word2vec_model.most_similar(positive=['summer', 'cold'], negative=['hot'], topn=1))\n",
        "print(\"Pretrained semantic similarities of the generated vectors\", pretrained_word2vec_model.most_similar(positive=[\"worse\", \"good\"], negative= [\"bad\"],topn=1))\n",
        "print(\"Pretrained semantic similarities of the generated vectors\", pretrained_word2vec_model.most_similar(positive=[\"dad\", \"daughter\"], negative= [\"mom\"], topn=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "- The Word2Vec model is trained on the processed review data from given dataset using the Word2Vec class from the Gensim library. \n",
        "You preprocess the text using a simple pre-processing function to convert it into a list of tokens.\n",
        "\n",
        "- Model Parameters: sepicified the parameters of the Word2Vec model, including the vector size (dimensionality of the word vectors),\n",
        "window size (maximum distance between the current and predicted word within a sentence), minimum word count (threshold for filtering infrequent words),\n",
        "and whether to use the Skip-gram (sg) or Continuous Bag of Words (CBOW) architecture.\n",
        "\n",
        "- Training: The Word2Vec model is trained on the processed review data to learn word embeddings. During training, the model iterates over\n",
        "the text corpus and adjusts the word vectors to maximize the likelihood of predicting context words given a target word \n",
        "(for the Skip-gram architecture)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "word #0/13518 is not\n",
            "word #1/13518 is work\n",
            "word #2/13518 is use\n",
            "word #3/13518 is printer\n",
            "word #4/13518 is one\n"
          ]
        }
      ],
      "source": [
        "# Own Word2Vec Model\n",
        "own_word2vec_model = Word2Vec(sentences=dataset.processed_review.apply(lambda x : utils.simple_preprocess(str(x))), vector_size=300, window=11, min_count=10, sg=1)\n",
        "\n",
        "# sample list of vocabulary\n",
        "for index, word in enumerate(own_word2vec_model.wv.index_to_key):\n",
        "    if index == 5:\n",
        "        break\n",
        "    print(f\"word #{index}/{len(own_word2vec_model.wv.index_to_key)} is {word}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Own Model semantic similarities of the generated vectors [('winter', 0.46683269739151)]\n",
            "Own Model semantic similarities of the generated vectors [('excellent', 0.4422725737094879)]\n",
            "Own Model semantic similarities of the generated vectors [('son', 0.5418736338615417)]\n"
          ]
        }
      ],
      "source": [
        "# Printing semantic similarities of the generated vectors\n",
        "print(\"Own Model semantic similarities of the generated vectors\", own_word2vec_model.wv.most_similar(positive=['summer', 'cold'], negative=['hot'], topn=1))\n",
        "print(\"Own Model semantic similarities of the generated vectors\", own_word2vec_model.wv.most_similar(positive=[\"worse\", \"good\"], negative= [\"bad\"], topn=1))\n",
        "print(\"Own Model semantic similarities of the generated vectors\", own_word2vec_model.wv.most_similar(positive=[\"dad\", \"daughter\"], negative= [\"mom\"], topn=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Comparison of Semantic Similarities:\n",
        "\n",
        "- The pretrained Word2Vec model tends to produce more intuitive semantic similarities between words compared to the Word2Vec model trained on your own dataset.\n",
        "\n",
        "- For example, in the comparison of \"summer - hot + cold = winter\", the pretrained model produces a semantic similarity with \"winter\", which aligns well with our expectations. However, the similarity score is relatively lower in the model trained on your own dataset.\n",
        "\n",
        "- Similarly, in the comparison of \"worse - bad + good = better\", the pretrained model again produces a semantic similarity with \"better\", whereas the model trained on your dataset returns \"excellent\" with a lower similarity score.\n",
        "\n",
        "- The same trend is observed in the comparison of \"dad - mom + daughter = son\", where the pretrained model provides a more relevant semantic similarity.\n",
        "\n",
        "#### Encoding Semantic Similarities:\n",
        "- The pretrained Word2Vec model, trained on a large corpus of diverse texts, seems to encode semantic similarities between words better. This is likely because the pretrained model has been trained on a vast amount of data, capturing richer semantic relationships.\n",
        "\n",
        "- On the other hand, the Word2Vec model trained on your own dataset may not have been exposed to as much diverse linguistic context, leading to less accurate semantic embeddings.\n",
        "\n",
        "<ins> In summary, the pretrained Word2Vec model generally performs better in encoding semantic similarities between words, likely due to its training on a larger and more diverse dataset. However, the model trained on your own dataset can still be valuable if it captures domain-specific nuances or vocabulary not present in the pretrained model.</ins>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(PearsonRResult(statistic=0.6238773487289394, pvalue=1.7963224351224885e-39), SignificanceResult(statistic=0.6589215888009288, pvalue=2.534605645914962e-45), 0.0)\n",
            "(PearsonRResult(statistic=0.42847274721379586, pvalue=4.32166669342491e-12), SignificanceResult(statistic=0.4206590906793289, pvalue=1.1477606232564647e-11), 32.29461756373937)\n"
          ]
        }
      ],
      "source": [
        "# evaluate both word2vec model for supporting our observation \n",
        "print(pretrained_word2vec_model.evaluate_word_pairs(datapath('wordsim353.tsv')))\n",
        "print(own_word2vec_model.wv.evaluate_word_pairs(datapath('wordsim353.tsv')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Simple models\n",
        "\n",
        "\n",
        "- The model_train_and_test function is designed to streamline the process of training and evaluating a SVM, Perceptron, Logistic regression, Naive Bayes model on both training and testing datasets.\n",
        "\n",
        "- Input Parameters:\n",
        "    - model: The machine learning model to be trained and evaluated.\n",
        "\n",
        "    - x_train: The feature matrix of the training dataset.\n",
        "    \n",
        "    - x_test: The feature matrix of the testing dataset.\n",
        "    \n",
        "    - y_train: The target labels of the training dataset.\n",
        "    \n",
        "    - y_test: The target labels of the testing dataset.\n",
        "    \n",
        "    - inputDatatype: A string indicating the type of input data (e.g., \"Word Embeddings\", \"TF-IDF Vectors\").\n",
        "    \n",
        "    - modelname: A string specifying the name of the model being used (e.g., \"Logistic Regression\", \"Random Forest\").\n",
        "    \n",
        "    - classification: A string indicating the type of classification task (e.g., \"Binary\", \"Multiclass\").\n",
        "    \n",
        "    - result: A list to store the results of model evaluation for later analysis.\n",
        "\n",
        "- Model Training: The function fits the specified model to the training data (x_train, y_train) using the fit method.\n",
        "\n",
        "- Training Accuracy: After training, the function calculates the accuracy of the model on the training data (x_train, y_train) using the accuracy_score function from scikit-learn. This accuracy score represents how well the model predicts the training data.\n",
        "\n",
        "- Testing Accuracy: The function then uses the trained model to make predictions on the testing data (x_test) and evaluates its performance using the corresponding ground truth labels (y_test). The accuracy of the model on the testing data is calculated using the accuracy_score function.\n",
        "\n",
        "- Result Logging: Appends the input data type, model name, classification type, training accuracy, and testing accuracy to the result list for further analysis or comparison with other models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "def model_train_and_test (model, x_train, x_test, y_train, y_test,inputDatatype, modelname, classification, result):\n",
        "    \n",
        "    model.fit(x_train, y_train) \n",
        "    train_pred = model.predict(x_train)\n",
        "    accuracy = accuracy_score(y_train, train_pred) \n",
        "\n",
        "    print(f'Training data Metrix ({modelname} - {inputDatatype} - {classification}):')\n",
        "    print(\"Accuracy :\", accuracy )\n",
        "\n",
        "    y_pred = model.predict(x_test)\n",
        "    accuracy_test = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f'Testing data Metrix ({modelname} - {inputDatatype} - {classification}):')\n",
        "    print(\"Accuracy :\", accuracy_test )\n",
        "    print()\n",
        "    # storing result in a list for better comparision and visualisation\n",
        "    result.append([inputDatatype, modelname,classification, accuracy, accuracy_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Creating result List to store data related to model\n",
        "result_column = [\"Input Data\", \"Model Name\", \"Classification\", \"Training Accuracy\", \"Testing Accuracy\"]\n",
        "result = list()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### This function performs TF-IDF vectorization on processed reviews, excluding those labeled as 3, utilizing a maximum of 5000 features, and returns a DataFrame containing TF-IDF values for each term in the reviews."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TF-IDF Vectorization\n",
        "def tfidf_vectorization(data):\n",
        "    tfidf = TfidfVectorizer(max_features=5000)\n",
        "    review_list = data[data.labels != 3].processed_review.tolist()\n",
        "    result = tfidf.fit_transform(review_list)\n",
        "    features = tfidf.get_feature_names_out()\n",
        "    tfidf_dataset = pd.DataFrame(data=result.toarray(), columns=features)\n",
        "    return tfidf_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### This function 'w2v_vectorize' takes a statement and a word embedding model, extracts word vectors for each word in the statement from the model, and returns the mean of these vectors as a single vector representation of the statement, or a zero vector if none of the words are found in the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vectorize function for Word Embedding\n",
        "def w2v_vectorize(statement, model):\n",
        "    words_vector = [model[word] for word in statement.split() if word in model]\n",
        "    if len(words_vector) == 0:\n",
        "        return np.zeros(300)\n",
        "    words_vector = np.array(words_vector)\n",
        "    return words_vector.mean(axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### This function takes a statement and a word embedding model, extracts word vectors for up to 10 words from the statement using the model, pads with zeros if necessary, and returns a flattened array representing the vectorized statement with a fixed length of 300 * max_words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vectorize function for Word Embedding with  max_words limit 10\n",
        "def w2v_vectorize_withLimit(statement, model, max_words = 10):\n",
        "    words_vector = [model[word] for word in statement.split() if word in model]\n",
        "    if len(words_vector) == 0:\n",
        "        return np.zeros(300 * max_words)\n",
        "    \n",
        "    words_vector = np.array(words_vector[:max_words])\n",
        "    padding_size = max_words - words_vector.shape[0]\n",
        "\n",
        "    if padding_size > 0:\n",
        "        padding = np.zeros((padding_size, words_vector.shape[1]))\n",
        "        words_vector = np.concatenate([words_vector, padding])\n",
        "\n",
        "    return words_vector.flatten()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Creating TF-IDF Dataset train test split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code first creates a TF-IDF representation of the dataset using a vectorization process. Then, it splits the dataset into training and testing sets, with 80% of the data used for training and 20% for testing, ensuring a consistent split across categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "#creating TF-IDF data set\n",
        "tfidf_dataset = tfidf_vectorization(dataset)\n",
        "# Train and Test split\n",
        "tfidf_x_train, tfidf_x_test, tfidf_y_train, tfidf_y_test = train_test_split(tfidf_dataset, dataset[dataset.labels != 3].labels, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Runing Simple models on TF-IDF data set "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data Metrix (Perceptron - TF-IDF - Binary):\n",
            "Accuracy : 0.8741375\n",
            "Testing data Metrix (Perceptron - TF-IDF - Binary):\n",
            "Accuracy : 0.8707\n",
            "\n",
            "Training data Metrix (SVM - TF-IDF - Binary):\n",
            "Accuracy : 0.92078125\n",
            "Testing data Metrix (SVM - TF-IDF - Binary):\n",
            "Accuracy : 0.90855\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Perceptron Model call for TF-IDF data set for binary classification\n",
        "perceptron = Perceptron(penalty='elasticnet', l1_ratio=0.8, alpha=1e-4, tol=1e-3)\n",
        "model_train_and_test(perceptron, tfidf_x_train, tfidf_x_test, tfidf_y_train, tfidf_y_test, \"TF-IDF\", \"Perceptron\", \"Binary\", result)\n",
        "\n",
        "# SVM Model call for TF-IDF data set for binary classification\n",
        "svm_model = svm.LinearSVC(dual=True, loss=\"hinge\", max_iter=200000, fit_intercept=True, tol=1e-5)\n",
        "model_train_and_test(svm_model, tfidf_x_train, tfidf_x_test, tfidf_y_train, tfidf_y_test, \"TF-IDF\", \"SVM\", \"Binary\", result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data Metrix (Logistic - TF-IDF - Binary):\n",
            "Accuracy : 0.916825\n",
            "Testing data Metrix (Logistic - TF-IDF - Binary):\n",
            "Accuracy : 0.907975\n",
            "\n",
            "Training data Metrix (Naive Bayes - TF-IDF - Binary):\n",
            "Accuracy : 0.86921875\n",
            "Testing data Metrix (Naive Bayes - TF-IDF - Binary):\n",
            "Accuracy : 0.8654\n",
            "\n"
          ]
        }
      ],
      "source": [
        "logistic_reg = LogisticRegression(max_iter=5000) \n",
        "model_train_and_test(logistic_reg, tfidf_x_train, tfidf_x_test, tfidf_y_train, tfidf_y_test, \"TF-IDF\", \"Logistic\", \"Binary\", result)\n",
        "\n",
        "naiveBayes = MultinomialNB()\n",
        "model_train_and_test(naiveBayes, tfidf_x_train, tfidf_x_test, tfidf_y_train, tfidf_y_test, \"TF-IDF\", \"Naive Bayes\", \"Binary\", result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "#free up memory ( beacuse kernal is crashing due to insuficient memory )\n",
        "del(index, word, dataframe, df, i, j, lemmatizer,perceptron, rating_stats, ratings, stopwords, svm_model,tfidf_dataset, \n",
        "    tfidf_x_train, tfidf_x_test, tfidf_y_train, tfidf_y_test,wn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Creating train test split for Binary classification.\n",
        "\n",
        "This code snippet splits a dataset into train and test sets for binary classification, excluding class label 3. It then vectorizes the textual data using word embeddings (Word2Vec) from two different models, both with and without limiting the maximum number of words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train and Test Split for Binary\n",
        "word2vec_binary_x_train, word2vec_binary_x_test, word2vec_binary_y_train, word2vec_binary_y_test = train_test_split(\n",
        "    dataset[dataset.labels != 3].processed_review, dataset[dataset.labels != 3].labels,\n",
        "    test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize for Word Embedding\n",
        "max_ = 10\n",
        "own_word2vec_binary_x_train = np.asarray([w2v_vectorize(statement=statement, model=own_word2vec_model.wv) for statement in word2vec_binary_x_train])\n",
        "own_word2vec_binary_x_test = np.asarray([w2v_vectorize(statement=statement, model=own_word2vec_model.wv) for statement in word2vec_binary_x_test])\n",
        "own_word2vec_limit_binary_x_train = np.asarray([w2v_vectorize_withLimit(statement=statement, model=own_word2vec_model.wv) for statement in word2vec_binary_x_train])\n",
        "own_word2vec_limit_binary_x_test = np.asarray([w2v_vectorize_withLimit(statement=statement, model=own_word2vec_model.wv) for statement in word2vec_binary_x_test])\n",
        "\n",
        "pretrained_word2vec_binary_x_train = np.asarray([w2v_vectorize(statement=statement, model=pretrained_word2vec_model) for statement in word2vec_binary_x_train])\n",
        "pretrained_word2vec_binary_x_test = np.asarray([w2v_vectorize(statement=statement, model=pretrained_word2vec_model) for statement in word2vec_binary_x_test])\n",
        "pretrained_word2vec_limit_binary_x_train = np.asarray([w2v_vectorize_withLimit(statement=statement, model=pretrained_word2vec_model) for statement in word2vec_binary_x_train])\n",
        "pretrained_word2vec_limit_binary_x_test = np.asarray([w2v_vectorize_withLimit(statement=statement, model=pretrained_word2vec_model) for statement in word2vec_binary_x_test])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Creating train test split for ternary cassification\n",
        "\n",
        "This code splits a dataset into training and testing sets, then converts textual data into word embeddings using two different Word2Vec models, one with no limit on vector size and another with a limit set to 10, for both self-trained and pre-trained Word2Vec models, effectively preparing the data for ternary classification tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train and Test Split for Ternary\n",
        "word2vec_ternary_x_train, word2vec_ternary_x_test, word2vec_ternary_y_train, word2vec_ternary_y_test = train_test_split(\n",
        "    dataset.processed_review, dataset.labels,\n",
        "    test_size=0.2, random_state=42)\n",
        "# Vectorize for Word Embedding\n",
        "max_ = 10\n",
        "own_word2vec_ternary_x_train = np.asarray([w2v_vectorize(statement=statement, model=own_word2vec_model.wv) for statement in word2vec_ternary_x_train])\n",
        "own_word2vec_ternary_x_test = np.asarray([w2v_vectorize(statement=statement, model=own_word2vec_model.wv) for statement in word2vec_ternary_x_test])\n",
        "own_word2vec_limit_ternary_x_train = np.asarray([w2v_vectorize_withLimit(statement=statement, model=own_word2vec_model.wv) for statement in word2vec_ternary_x_train])\n",
        "own_word2vec_limit_ternary_x_test = np.asarray([w2v_vectorize_withLimit(statement=statement, model=own_word2vec_model.wv) for statement in word2vec_ternary_x_test])\n",
        "\n",
        "pretrained_word2vec_ternary_x_train = np.asarray([w2v_vectorize(statement=statement, model=pretrained_word2vec_model) for statement in word2vec_ternary_x_train])\n",
        "pretrained_word2vec_ternary_x_test = np.asarray([w2v_vectorize(statement=statement, model=pretrained_word2vec_model) for statement in word2vec_ternary_x_test])\n",
        "pretrained_word2vec_limit_ternary_x_train = np.asarray([w2v_vectorize_withLimit(statement=statement, model=pretrained_word2vec_model) for statement in word2vec_ternary_x_train])\n",
        "pretrained_word2vec_limit_ternary_x_test = np.asarray([w2v_vectorize_withLimit(statement=statement, model=pretrained_word2vec_model) for statement in word2vec_ternary_x_test])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Runing Perceptron SVM and logistic regression model for Binary classification\n",
        "\n",
        "code define and train two Perceptron models and two Support Vector Machine (SVM) models for binary classification tasks using custom and pre-trained Word2Vec embeddings, with specific hyperparameters and training settings. The models are evaluated and compared using the model_train_and_test function, storing the results for analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data Metrix (Perceptron - Word2Vec-custom - Binary):\n",
            "Accuracy : 0.85066875\n",
            "Testing data Metrix (Perceptron - Word2Vec-custom - Binary):\n",
            "Accuracy : 0.8501\n",
            "\n",
            "Training data Metrix (SVM - Word2Vec-custom - Binary):\n",
            "Accuracy : 0.8824375\n",
            "Testing data Metrix (SVM - Word2Vec-custom - Binary):\n",
            "Accuracy : 0.880775\n",
            "\n",
            "Training data Metrix (Perceptron - Word2Vec-Pre-Trained - Binary):\n",
            "Accuracy : 0.83849375\n",
            "Testing data Metrix (Perceptron - Word2Vec-Pre-Trained - Binary):\n",
            "Accuracy : 0.838725\n",
            "\n",
            "Training data Metrix (SVM - Word2Vec-Pre-Trained - Binary):\n",
            "Accuracy : 0.86276875\n",
            "Testing data Metrix (SVM - Word2Vec-Pre-Trained - Binary):\n",
            "Accuracy : 0.858525\n",
            "\n"
          ]
        }
      ],
      "source": [
        "perceptron_own = Perceptron(penalty='elasticnet', l1_ratio=0.8, alpha=1e-4, tol=1e-3)\n",
        "model_train_and_test(perceptron_own, own_word2vec_binary_x_train, own_word2vec_binary_x_test, word2vec_binary_y_train, word2vec_binary_y_test, \"Word2Vec-custom\", \"Perceptron\", \"Binary\", result)\n",
        "\n",
        "svm_model_own = svm.LinearSVC(dual=True, loss=\"hinge\", max_iter=200000, fit_intercept=True, tol=1e-5)\n",
        "model_train_and_test(svm_model_own, own_word2vec_binary_x_train, own_word2vec_binary_x_test, word2vec_binary_y_train, word2vec_binary_y_test, \"Word2Vec-custom\", \"SVM\", \"Binary\", result)\n",
        "\n",
        "perceptron_pretrained = Perceptron(penalty='elasticnet', l1_ratio=0.8, alpha=1e-4, tol=1e-3)\n",
        "model_train_and_test(perceptron_pretrained, pretrained_word2vec_binary_x_train, pretrained_word2vec_binary_x_test, word2vec_binary_y_train, word2vec_binary_y_test, \"Word2Vec-Pre-Trained\", \"Perceptron\", \"Binary\", result)\n",
        "\n",
        "svm_model_pretrained = svm.LinearSVC(dual=True, loss='hinge', max_iter=200000, fit_intercept=True, tol=1e-5)\n",
        "model_train_and_test(svm_model_pretrained, pretrained_word2vec_binary_x_train, pretrained_word2vec_binary_x_test, word2vec_binary_y_train, word2vec_binary_y_test, \"Word2Vec-Pre-Trained\", \"SVM\", \"Binary\", result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data Metrix (Logistic - Word2Vec-custom - Binary):\n",
            "Accuracy : 0.88164375\n",
            "Testing data Metrix (Logistic - Word2Vec-custom - Binary):\n",
            "Accuracy : 0.880675\n",
            "\n",
            "Training data Metrix (Logistic - Word2Vec-Pre-Trained - Binary):\n",
            "Accuracy : 0.861375\n",
            "Testing data Metrix (Logistic - Word2Vec-Pre-Trained - Binary):\n",
            "Accuracy : 0.8574\n",
            "\n"
          ]
        }
      ],
      "source": [
        "logistic_own = LogisticRegression(max_iter=5000) \n",
        "model_train_and_test(logistic_own, own_word2vec_binary_x_train, own_word2vec_binary_x_test, word2vec_binary_y_train, word2vec_binary_y_test, \"Word2Vec-custom\", \"Logistic\", \"Binary\", result)\n",
        "\n",
        "logistic_pretrained = LogisticRegression(max_iter=5000) \n",
        "model_train_and_test(logistic_pretrained, pretrained_word2vec_binary_x_train, pretrained_word2vec_binary_x_test, word2vec_binary_y_train, word2vec_binary_y_test, \"Word2Vec-Pre-Trained\", \"Logistic\", \"Binary\", result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### FNN Model\n",
        "\n",
        "Function mlp_classification for training and evaluating a multi-layer perceptron (MLP) model for classification tasks using PyTorch. Here's a concise summary:\n",
        "\n",
        "- Data Preparation: The function takes training and testing data along with their labels, as well as parameters related to the network architecture and training settings.\n",
        "\n",
        "- Model Definition: Inside the function, an MLP model is defined using the nn.Module class. The architecture is specified by the input_size, hidden_sizes, and output_size parameters.\n",
        "\n",
        "- ReLU activation functions are used between layers.\n",
        "\n",
        "- Training Loop: The function then trains the model using the provided training data. It utilizes the Adam optimizer and Cross Entropy Loss criterion for optimization.\n",
        "\n",
        "- Testing Loop: After training, the function evaluates the model's performance on the testing data.\n",
        "\n",
        "- Results: Finally, the function prints the training loss for each epoch and the testing accuracy. Additionally, it appends the results to a list called result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mlp_classification(X_train, y_train, X_test, y_test, input_size, hidden_sizes, output_size, input_data, model_name, classification, batch_size=64, epochs=25, learning_rate=0.001, ):\n",
        "    # Convert data to PyTorch tensors\n",
        "    X_train_tensor = torch.from_numpy(X_train).to(dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train.values - 1, dtype=torch.long)\n",
        "    X_test_tensor = torch.from_numpy(X_test).to(dtype=torch.float32)\n",
        "    y_test_tensor = torch.tensor(y_test.values - 1, dtype=torch.long)\n",
        "\n",
        "    # Create DataLoader\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Define the MLP model\n",
        "    class MLP(nn.Module):\n",
        "        def __init__(self, input_size, hidden_sizes, output_size):\n",
        "            super(MLP, self).__init__()\n",
        "            layers = []\n",
        "            layers.append(nn.Flatten())\n",
        "            for i in range(len(hidden_sizes)):\n",
        "                layers.append(nn.Linear(input_size if i == 0 else hidden_sizes[i-1], hidden_sizes[i]))\n",
        "                layers.append(nn.ReLU())\n",
        "            layers.append(nn.Linear(hidden_sizes[-1], output_size))\n",
        "            self.model = nn.Sequential(*layers)\n",
        "\n",
        "        def forward(self, x):\n",
        "            return self.model(x)\n",
        "\n",
        "    # Create model, criterion, and optimizer\n",
        "    model = MLP(input_size, hidden_sizes, output_size)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Loss monitoring\n",
        "    training_losses = []\n",
        "    print(f'\\n\\nTraining ({model_name}) - ({input_data}) - ({classification}) : \\n')\n",
        "    print('Model configuration :')\n",
        "    print( model )\n",
        "    print(f'\\nTraining Loss for each epoch :')\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            # print(outputs.size(), labels.size())\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        # Average loss for the epoch\n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        training_losses.append(avg_loss)\n",
        "        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {avg_loss:.4f}')\n",
        "\n",
        "    # Testing loop\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            # print(torch.max(outputs.data, 1))\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            # print(_, predicted.size())\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    testing_accuracy = correct / total\n",
        "    print(f'Testing Accuracy for ({model_name}) - ({input_data}) - ({classification}) : {testing_accuracy}')\n",
        "    result.append([input_data, model_name, classification, \"-\", testing_accuracy])\n",
        "    return model, testing_accuracy, training_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Running FNN model for binary cclassification\n",
        "\n",
        "conducting multi-layer perceptron (MLP) binary classification experiments using different word embedding approaches (custom vs. pretrained Word2Vec embeddings) and different input sizes (300 vs. 3000)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Training (FNN - (Average)) - (Word2Vec-custom) - (Binary) : \n",
            "\n",
            "Model configuration :\n",
            "MLP(\n",
            "  (model): Sequential(\n",
            "    (0): Flatten(start_dim=1, end_dim=-1)\n",
            "    (1): Linear(in_features=300, out_features=50, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=50, out_features=10, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Linear(in_features=10, out_features=2, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "Training Loss for each epoch :\n",
            "Epoch [1/25], Loss: 0.3151\n",
            "Epoch [2/25], Loss: 0.2786\n",
            "Epoch [3/25], Loss: 0.2660\n",
            "Epoch [4/25], Loss: 0.2583\n",
            "Epoch [5/25], Loss: 0.2525\n",
            "Epoch [6/25], Loss: 0.2479\n",
            "Epoch [7/25], Loss: 0.2436\n",
            "Epoch [8/25], Loss: 0.2392\n",
            "Epoch [9/25], Loss: 0.2358\n",
            "Epoch [10/25], Loss: 0.2325\n",
            "Epoch [11/25], Loss: 0.2295\n",
            "Epoch [12/25], Loss: 0.2271\n",
            "Epoch [13/25], Loss: 0.2249\n",
            "Epoch [14/25], Loss: 0.2218\n",
            "Epoch [15/25], Loss: 0.2197\n",
            "Epoch [16/25], Loss: 0.2185\n",
            "Epoch [17/25], Loss: 0.2160\n",
            "Epoch [18/25], Loss: 0.2147\n",
            "Epoch [19/25], Loss: 0.2127\n",
            "Epoch [20/25], Loss: 0.2108\n",
            "Epoch [21/25], Loss: 0.2088\n",
            "Epoch [22/25], Loss: 0.2079\n",
            "Epoch [23/25], Loss: 0.2058\n",
            "Epoch [24/25], Loss: 0.2045\n",
            "Epoch [25/25], Loss: 0.2033\n",
            "Testing Accuracy for (FNN - (Average)) - (Word2Vec-custom) - (Binary) : 0.899075\n",
            "\n",
            "\n",
            "Training (FNN - (Average)) - (Word2Vec-Pretrained) - (Binary) : \n",
            "\n",
            "Model configuration :\n",
            "MLP(\n",
            "  (model): Sequential(\n",
            "    (0): Flatten(start_dim=1, end_dim=-1)\n",
            "    (1): Linear(in_features=300, out_features=50, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=50, out_features=10, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Linear(in_features=10, out_features=2, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "Training Loss for each epoch :\n",
            "Epoch [1/25], Loss: 0.3445\n",
            "Epoch [2/25], Loss: 0.2937\n",
            "Epoch [3/25], Loss: 0.2816\n",
            "Epoch [4/25], Loss: 0.2741\n",
            "Epoch [5/25], Loss: 0.2682\n",
            "Epoch [6/25], Loss: 0.2626\n",
            "Epoch [7/25], Loss: 0.2585\n",
            "Epoch [8/25], Loss: 0.2548\n",
            "Epoch [9/25], Loss: 0.2510\n",
            "Epoch [10/25], Loss: 0.2477\n",
            "Epoch [11/25], Loss: 0.2441\n",
            "Epoch [12/25], Loss: 0.2423\n",
            "Epoch [13/25], Loss: 0.2391\n",
            "Epoch [14/25], Loss: 0.2369\n",
            "Epoch [15/25], Loss: 0.2346\n",
            "Epoch [16/25], Loss: 0.2326\n",
            "Epoch [17/25], Loss: 0.2308\n",
            "Epoch [18/25], Loss: 0.2293\n",
            "Epoch [19/25], Loss: 0.2271\n",
            "Epoch [20/25], Loss: 0.2249\n",
            "Epoch [21/25], Loss: 0.2236\n",
            "Epoch [22/25], Loss: 0.2222\n",
            "Epoch [23/25], Loss: 0.2209\n",
            "Epoch [24/25], Loss: 0.2197\n",
            "Epoch [25/25], Loss: 0.2180\n",
            "Testing Accuracy for (FNN - (Average)) - (Word2Vec-Pretrained) - (Binary) : 0.8876\n",
            "\n",
            "\n",
            "Training (FNN - (Concat)) - (Word2Vec-custom) - (Binary) : \n",
            "\n",
            "Model configuration :\n",
            "MLP(\n",
            "  (model): Sequential(\n",
            "    (0): Flatten(start_dim=1, end_dim=-1)\n",
            "    (1): Linear(in_features=3000, out_features=50, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=50, out_features=10, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Linear(in_features=10, out_features=2, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "Training Loss for each epoch :\n",
            "Epoch [1/25], Loss: 0.3078\n",
            "Epoch [2/25], Loss: 0.2678\n",
            "Epoch [3/25], Loss: 0.2423\n",
            "Epoch [4/25], Loss: 0.2157\n",
            "Epoch [5/25], Loss: 0.1860\n",
            "Epoch [6/25], Loss: 0.1567\n",
            "Epoch [7/25], Loss: 0.1301\n",
            "Epoch [8/25], Loss: 0.1068\n",
            "Epoch [9/25], Loss: 0.0894\n",
            "Epoch [10/25], Loss: 0.0769\n",
            "Epoch [11/25], Loss: 0.0656\n",
            "Epoch [12/25], Loss: 0.0585\n",
            "Epoch [13/25], Loss: 0.0521\n",
            "Epoch [14/25], Loss: 0.0454\n",
            "Epoch [15/25], Loss: 0.0429\n",
            "Epoch [16/25], Loss: 0.0385\n",
            "Epoch [17/25], Loss: 0.0352\n",
            "Epoch [18/25], Loss: 0.0326\n",
            "Epoch [19/25], Loss: 0.0289\n",
            "Epoch [20/25], Loss: 0.0279\n",
            "Epoch [21/25], Loss: 0.0261\n",
            "Epoch [22/25], Loss: 0.0246\n",
            "Epoch [23/25], Loss: 0.0230\n",
            "Epoch [24/25], Loss: 0.0231\n",
            "Epoch [25/25], Loss: 0.0202\n",
            "Testing Accuracy for (FNN - (Concat)) - (Word2Vec-custom) - (Binary) : 0.857075\n",
            "\n",
            "\n",
            "Training (FNN - (Concat)) - (Word2Vec-Pretrained) - (Binary) : \n",
            "\n",
            "Model configuration :\n",
            "MLP(\n",
            "  (model): Sequential(\n",
            "    (0): Flatten(start_dim=1, end_dim=-1)\n",
            "    (1): Linear(in_features=3000, out_features=50, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=50, out_features=10, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Linear(in_features=10, out_features=2, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "Training Loss for each epoch :\n",
            "Epoch [1/25], Loss: 0.3300\n",
            "Epoch [2/25], Loss: 0.2773\n",
            "Epoch [3/25], Loss: 0.2466\n",
            "Epoch [4/25], Loss: 0.2166\n",
            "Epoch [5/25], Loss: 0.1851\n",
            "Epoch [6/25], Loss: 0.1552\n",
            "Epoch [7/25], Loss: 0.1286\n",
            "Epoch [8/25], Loss: 0.1071\n",
            "Epoch [9/25], Loss: 0.0884\n",
            "Epoch [10/25], Loss: 0.0769\n",
            "Epoch [11/25], Loss: 0.0651\n",
            "Epoch [12/25], Loss: 0.0582\n",
            "Epoch [13/25], Loss: 0.0524\n",
            "Epoch [14/25], Loss: 0.0475\n",
            "Epoch [15/25], Loss: 0.0414\n",
            "Epoch [16/25], Loss: 0.0396\n",
            "Epoch [17/25], Loss: 0.0357\n",
            "Epoch [18/25], Loss: 0.0345\n",
            "Epoch [19/25], Loss: 0.0329\n",
            "Epoch [20/25], Loss: 0.0297\n",
            "Epoch [21/25], Loss: 0.0298\n",
            "Epoch [22/25], Loss: 0.0265\n",
            "Epoch [23/25], Loss: 0.0254\n",
            "Epoch [24/25], Loss: 0.0254\n",
            "Epoch [25/25], Loss: 0.0240\n",
            "Testing Accuracy for (FNN - (Concat)) - (Word2Vec-Pretrained) - (Binary) : 0.849725\n"
          ]
        }
      ],
      "source": [
        "hidden_sizes = [50, 10]\n",
        "output_size = 2\n",
        "input_size = 300\n",
        "model, accuracy, training_loss = mlp_classification(own_word2vec_binary_x_train, word2vec_binary_y_train, own_word2vec_binary_x_test, word2vec_binary_y_test,input_size, hidden_sizes, output_size, \"Word2Vec-custom\", \"FNN - (Average)\", \"Binary\" )\n",
        "model, accuracy, training_loss = mlp_classification(pretrained_word2vec_binary_x_train, word2vec_binary_y_train, pretrained_word2vec_binary_x_test, word2vec_binary_y_test,input_size, hidden_sizes, output_size, \"Word2Vec-Pretrained\", \"FNN - (Average)\", \"Binary\")\n",
        "\n",
        "input_size = 3000\n",
        "model, accuracy, training_loss = mlp_classification(own_word2vec_limit_binary_x_train, word2vec_binary_y_train, own_word2vec_limit_binary_x_test, word2vec_binary_y_test,input_size, hidden_sizes, output_size, \"Word2Vec-custom\", \"FNN - (Concat)\", \"Binary\")\n",
        "model, accuracy, training_loss = mlp_classification(pretrained_word2vec_limit_binary_x_train, word2vec_binary_y_train, pretrained_word2vec_limit_binary_x_test, word2vec_binary_y_test,input_size, hidden_sizes, output_size, \"Word2Vec-Pretrained\", \"FNN - (Concat)\", \"Binary\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "conducting multi-layer perceptron (MLP) ternary classification experiments using different word embedding approaches (custom vs. pretrained Word2Vec embeddings) and different input sizes (300 vs. 3000)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Training (FNN - (Average)) - (Word2Vec-Custom) - (Ternary) : \n",
            "\n",
            "Model configuration :\n",
            "MLP(\n",
            "  (model): Sequential(\n",
            "    (0): Flatten(start_dim=1, end_dim=-1)\n",
            "    (1): Linear(in_features=300, out_features=50, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=50, out_features=10, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Linear(in_features=10, out_features=3, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "Training Loss for each epoch :\n",
            "Epoch [1/25], Loss: 0.6634\n",
            "Epoch [2/25], Loss: 0.6043\n",
            "Epoch [3/25], Loss: 0.5861\n",
            "Epoch [4/25], Loss: 0.5768\n",
            "Epoch [5/25], Loss: 0.5694\n",
            "Epoch [6/25], Loss: 0.5634\n",
            "Epoch [7/25], Loss: 0.5586\n",
            "Epoch [8/25], Loss: 0.5540\n",
            "Epoch [9/25], Loss: 0.5503\n",
            "Epoch [10/25], Loss: 0.5470\n",
            "Epoch [11/25], Loss: 0.5436\n",
            "Epoch [12/25], Loss: 0.5411\n",
            "Epoch [13/25], Loss: 0.5377\n",
            "Epoch [14/25], Loss: 0.5355\n",
            "Epoch [15/25], Loss: 0.5333\n",
            "Epoch [16/25], Loss: 0.5317\n",
            "Epoch [17/25], Loss: 0.5289\n",
            "Epoch [18/25], Loss: 0.5271\n",
            "Epoch [19/25], Loss: 0.5256\n",
            "Epoch [20/25], Loss: 0.5242\n",
            "Epoch [21/25], Loss: 0.5228\n",
            "Epoch [22/25], Loss: 0.5214\n",
            "Epoch [23/25], Loss: 0.5201\n",
            "Epoch [24/25], Loss: 0.5183\n",
            "Epoch [25/25], Loss: 0.5175\n",
            "Testing Accuracy for (FNN - (Average)) - (Word2Vec-Custom) - (Ternary) : 0.76212\n",
            "\n",
            "\n",
            "Training (FNN - (Average)) - (Word2Vec-Pretrained) - (Ternary) : \n",
            "\n",
            "Model configuration :\n",
            "MLP(\n",
            "  (model): Sequential(\n",
            "    (0): Flatten(start_dim=1, end_dim=-1)\n",
            "    (1): Linear(in_features=300, out_features=50, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=50, out_features=10, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Linear(in_features=10, out_features=3, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "Training Loss for each epoch :\n",
            "Epoch [1/25], Loss: 0.7334\n",
            "Epoch [2/25], Loss: 0.6567\n",
            "Epoch [3/25], Loss: 0.6325\n",
            "Epoch [4/25], Loss: 0.6197\n",
            "Epoch [5/25], Loss: 0.6110\n",
            "Epoch [6/25], Loss: 0.6046\n",
            "Epoch [7/25], Loss: 0.5985\n",
            "Epoch [8/25], Loss: 0.5941\n",
            "Epoch [9/25], Loss: 0.5899\n",
            "Epoch [10/25], Loss: 0.5866\n",
            "Epoch [11/25], Loss: 0.5831\n",
            "Epoch [12/25], Loss: 0.5804\n",
            "Epoch [13/25], Loss: 0.5781\n",
            "Epoch [14/25], Loss: 0.5756\n",
            "Epoch [15/25], Loss: 0.5731\n",
            "Epoch [16/25], Loss: 0.5712\n",
            "Epoch [17/25], Loss: 0.5692\n",
            "Epoch [18/25], Loss: 0.5668\n",
            "Epoch [19/25], Loss: 0.5657\n",
            "Epoch [20/25], Loss: 0.5644\n",
            "Epoch [21/25], Loss: 0.5624\n",
            "Epoch [22/25], Loss: 0.5611\n",
            "Epoch [23/25], Loss: 0.5599\n",
            "Epoch [24/25], Loss: 0.5581\n",
            "Epoch [25/25], Loss: 0.5573\n",
            "Testing Accuracy for (FNN - (Average)) - (Word2Vec-Pretrained) - (Ternary) : 0.74286\n",
            "\n",
            "\n",
            "Training (FNN - (Concat)) - (Word2Vec-Custom) - (Ternary) : \n",
            "\n",
            "Model configuration :\n",
            "MLP(\n",
            "  (model): Sequential(\n",
            "    (0): Flatten(start_dim=1, end_dim=-1)\n",
            "    (1): Linear(in_features=3000, out_features=50, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=50, out_features=10, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Linear(in_features=10, out_features=3, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "Training Loss for each epoch :\n",
            "Epoch [1/25], Loss: 0.6409\n",
            "Epoch [2/25], Loss: 0.5833\n",
            "Epoch [3/25], Loss: 0.5566\n",
            "Epoch [4/25], Loss: 0.5299\n",
            "Epoch [5/25], Loss: 0.5031\n",
            "Epoch [6/25], Loss: 0.4754\n",
            "Epoch [7/25], Loss: 0.4495\n",
            "Epoch [8/25], Loss: 0.4243\n",
            "Epoch [9/25], Loss: 0.4023\n",
            "Epoch [10/25], Loss: 0.3810\n",
            "Epoch [11/25], Loss: 0.3626\n",
            "Epoch [12/25], Loss: 0.3448\n",
            "Epoch [13/25], Loss: 0.3295\n",
            "Epoch [14/25], Loss: 0.3146\n",
            "Epoch [15/25], Loss: 0.3011\n",
            "Epoch [16/25], Loss: 0.2885\n",
            "Epoch [17/25], Loss: 0.2766\n",
            "Epoch [18/25], Loss: 0.2667\n",
            "Epoch [19/25], Loss: 0.2577\n",
            "Epoch [20/25], Loss: 0.2478\n",
            "Epoch [21/25], Loss: 0.2394\n",
            "Epoch [22/25], Loss: 0.2307\n",
            "Epoch [23/25], Loss: 0.2237\n",
            "Epoch [24/25], Loss: 0.2162\n",
            "Epoch [25/25], Loss: 0.2103\n",
            "Testing Accuracy for (FNN - (Concat)) - (Word2Vec-Custom) - (Ternary) : 0.69678\n",
            "\n",
            "\n",
            "Training (FNN - (Concat)) - (Word2Vec-Pretrained) - (Ternary) : \n",
            "\n",
            "Model configuration :\n",
            "MLP(\n",
            "  (model): Sequential(\n",
            "    (0): Flatten(start_dim=1, end_dim=-1)\n",
            "    (1): Linear(in_features=3000, out_features=50, bias=True)\n",
            "    (2): ReLU()\n",
            "    (3): Linear(in_features=50, out_features=10, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Linear(in_features=10, out_features=3, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "Training Loss for each epoch :\n",
            "Epoch [1/25], Loss: 0.6662\n",
            "Epoch [2/25], Loss: 0.6006\n",
            "Epoch [3/25], Loss: 0.5685\n",
            "Epoch [4/25], Loss: 0.5386\n",
            "Epoch [5/25], Loss: 0.5082\n",
            "Epoch [6/25], Loss: 0.4798\n",
            "Epoch [7/25], Loss: 0.4527\n",
            "Epoch [8/25], Loss: 0.4285\n",
            "Epoch [9/25], Loss: 0.4061\n",
            "Epoch [10/25], Loss: 0.3851\n",
            "Epoch [11/25], Loss: 0.3669\n",
            "Epoch [12/25], Loss: 0.3489\n",
            "Epoch [13/25], Loss: 0.3340\n",
            "Epoch [14/25], Loss: 0.3202\n",
            "Epoch [15/25], Loss: 0.3074\n",
            "Epoch [16/25], Loss: 0.2960\n",
            "Epoch [17/25], Loss: 0.2857\n",
            "Epoch [18/25], Loss: 0.2762\n",
            "Epoch [19/25], Loss: 0.2662\n",
            "Epoch [20/25], Loss: 0.2584\n",
            "Epoch [21/25], Loss: 0.2511\n",
            "Epoch [22/25], Loss: 0.2437\n",
            "Epoch [23/25], Loss: 0.2366\n",
            "Epoch [24/25], Loss: 0.2301\n",
            "Epoch [25/25], Loss: 0.2245\n",
            "Testing Accuracy for (FNN - (Concat)) - (Word2Vec-Pretrained) - (Ternary) : 0.6867\n"
          ]
        }
      ],
      "source": [
        "hidden_sizes = [50, 10]\n",
        "output_size = 3\n",
        "input_size = 300\n",
        "model, accuracy, training_loss = mlp_classification(own_word2vec_ternary_x_train, word2vec_ternary_y_train, own_word2vec_ternary_x_test, word2vec_ternary_y_test,input_size, hidden_sizes, output_size,\"Word2Vec-Custom\", \"FNN - (Average)\", \"Ternary\")\n",
        "model, accuracy, training_loss = mlp_classification(pretrained_word2vec_ternary_x_train, word2vec_ternary_y_train, pretrained_word2vec_ternary_x_test, word2vec_ternary_y_test,input_size, hidden_sizes, output_size, \"Word2Vec-Pretrained\", \"FNN - (Average)\", \"Ternary\")\n",
        "\n",
        "input_size = 3000\n",
        "model, accuracy, training_loss = mlp_classification(own_word2vec_limit_ternary_x_train, word2vec_ternary_y_train, own_word2vec_limit_ternary_x_test, word2vec_ternary_y_test,input_size, hidden_sizes, output_size, \"Word2Vec-Custom\", \"FNN - (Concat)\", \"Ternary\")\n",
        "model, accuracy, training_loss = mlp_classification(pretrained_word2vec_limit_ternary_x_train, word2vec_ternary_y_train, pretrained_word2vec_limit_ternary_x_test, word2vec_ternary_y_test,input_size, hidden_sizes, output_size, \"Word2Vec-Pretrained\", \"FNN - (Concat)\", \"Ternary\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "#free up memory\n",
        "del(   training_loss, svm_model_pretrained, svm_model_own, pretrained_word2vec_ternary_x_train, pretrained_word2vec_limit_ternary_x_train, \n",
        "    pretrained_word2vec_ternary_x_test, pretrained_word2vec_limit_binary_x_train, pretrained_word2vec_binary_x_train, pretrained_word2vec_limit_binary_x_test,\n",
        "    pretrained_word2vec_binary_x_test, perceptron_own, perceptron_pretrained, own_word2vec_ternary_x_test,\n",
        " own_word2vec_ternary_x_train, pretrained_word2vec_limit_ternary_x_test,accuracy, hidden_sizes, input_size, max_, model, output_size,own_word2vec_binary_x_test, own_word2vec_binary_x_train, \n",
        "    own_word2vec_limit_ternary_x_test, own_word2vec_limit_ternary_x_train, own_word2vec_limit_binary_x_train,own_word2vec_limit_binary_x_test\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [],
      "source": [
        "def func( statement, model, max_review_length, vector_size):\n",
        "    # Load and preprocess data for a single sample\n",
        "    words_vector = [model[word] for word in statement.split() if word in model]\n",
        "    if len(words_vector) == 0:\n",
        "        words_vector = torch.zeros(max_review_length, vector_size)\n",
        "    else:\n",
        "        words_vector = torch.tensor(words_vector[:max_review_length])\n",
        "\n",
        "    # Pad or truncate the sequence\n",
        "    if words_vector.shape[0] < max_review_length:\n",
        "        padding_size = max_review_length - words_vector.shape[0]\n",
        "        padding = torch.zeros(padding_size, vector_size)\n",
        "        words_vector = torch.cat([words_vector, padding])\n",
        "\n",
        "    return words_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### CNN Model\n",
        "\n",
        "This Python function defines and trains a Convolutional Neural Network (CNN) model for text classification using PyTorch. Here's a summary of what the function does:\n",
        "\n",
        "- Preprocessing: The function first limits the length of input text sequences to 50 words and converts them into PyTorch tensors.\n",
        "\n",
        "- Model Architecture: It defines a CNN model using nn.Sequential, consisting of two convolutional layers followed by ReLU activation functions and a fully connected layer for classification.\n",
        "\n",
        "- Training: The model is trained using the Adam optimizer and Cross Entropy Loss for a specified number of epochs. Training progress is monitored, and training losses for each epoch are printed.\n",
        "\n",
        "- Testing: The trained model is evaluated on the test data to compute the accuracy. The function returns the trained model, testing accuracy, and training losses.\n",
        "\n",
        "- Output: The function prints out the model configuration and training losses for each epoch during training. Finally, it appends the test accuracy to the provided result list along with input data details and model configuration for further analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "def CNN_Model(x_train, y_train, x_test, y_test, vector_size, output_channels1, output_channels2, num_classes, word2vec,\n",
        "              input_data, model_name, classification, result, max_review_length=50, batch_size=32, epochs=10, learning_rate=0.001):\n",
        "    #  Concat first 50 words \n",
        "    x_train_limit = np.asarray([func(statement=str(statement), model=word2vec, max_review_length=50, vector_size=300 ) for statement in x_train])\n",
        "    x_test_limit = np.asarray([func(statement=str(statement), model=word2vec, max_review_length=50, vector_size=300 ) for statement in x_test])\n",
        "    \n",
        "    # Convert data to PyTorch tensors\n",
        "    x_train_tensor = torch.from_numpy(x_train_limit).to(dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train.values - 1, dtype=torch.long)\n",
        "    x_test_tensor = torch.from_numpy(x_test_limit).to(dtype=torch.float32)\n",
        "    y_test_tensor = torch.tensor(y_test.values - 1, dtype=torch.long)\n",
        "\n",
        "    # Create DataLoader\n",
        "    train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "    test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Define the CNN model using Sequential\n",
        "    model = nn.Sequential(\n",
        "        nn.Conv1d(in_channels=vector_size, out_channels=output_channels1, kernel_size=3),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv1d(in_channels=output_channels1, out_channels=output_channels2, kernel_size=3),\n",
        "        nn.ReLU(),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(output_channels2 * (max_review_length - 4), num_classes)\n",
        "    )\n",
        "\n",
        "    # Loss and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Loss monitoring\n",
        "    training_losses = []\n",
        "    print(f'\\n\\nTraining ({model_name}) - ({input_data}) - ({classification}) : \\n')\n",
        "    print('Model configuration :')\n",
        "    print( model )\n",
        "    print(f'\\nTraining Loss for each epoch :')\n",
        "    # Training loop\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        epoch_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            # No need to check for sparse input, as Conv1d layer expects dense input\n",
        "            outputs = model(inputs.permute(0, 2, 1))\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        # Average loss for the epoch\n",
        "        avg_loss = epoch_loss / len(train_loader)\n",
        "        training_losses.append(avg_loss)\n",
        "        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {avg_loss:.4f}')\n",
        "\n",
        "    # Testing loop\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs.permute(0, 2, 1))\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    testing_accuracy = correct / total\n",
        "    result.append([input_data, model_name, classification, \"-\", testing_accuracy])\n",
        "    return model, testing_accuracy, training_losses\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Below snippet utilizes the CNN_Model function to train a Convolutional Neural Network (CNN) model for binary classification using word embeddings generated by a custom Word2Vec model (own_word2vec_model):\n",
        "\n",
        "- Model Configuration: The CNN model is configured with an input vector size of 300 (corresponding to the dimensionality of the Word2Vec embeddings), 50 output channels for the first convolutional layer, 10 output channels for the second convolutional layer, and an output size of 2 for binary classification.\n",
        "\n",
        "- Training: The CNN_Model function is invoked with the training and testing data, along with the specified model configuration. The function trains the CNN model on the provided training data, monitors the training progress, and computes the testing accuracy on the test data.\n",
        "\n",
        "- Output: After training, the function prints the test accuracy for binary classification. This accuracy reflects how well the CNN model can classify binary sentiment labels based on the provided Word2Vec embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Training (CNN - (50 concat)) - (Word2Vec-custom) - (Binary) : \n",
            "\n",
            "Model configuration :\n",
            "Sequential(\n",
            "  (0): Conv1d(300, 50, kernel_size=(3,), stride=(1,))\n",
            "  (1): ReLU()\n",
            "  (2): Conv1d(50, 10, kernel_size=(3,), stride=(1,))\n",
            "  (3): ReLU()\n",
            "  (4): Flatten(start_dim=1, end_dim=-1)\n",
            "  (5): Linear(in_features=460, out_features=2, bias=True)\n",
            ")\n",
            "\n",
            "Training Loss for each epoch :\n",
            "Epoch [1/10], Loss: 0.2430\n",
            "Epoch [2/10], Loss: 0.1913\n",
            "Epoch [3/10], Loss: 0.1739\n",
            "Epoch [4/10], Loss: 0.1585\n",
            "Epoch [5/10], Loss: 0.1460\n",
            "Epoch [6/10], Loss: 0.1338\n",
            "Epoch [7/10], Loss: 0.1220\n",
            "Epoch [8/10], Loss: 0.1126\n",
            "Epoch [9/10], Loss: 0.1037\n",
            "Epoch [10/10], Loss: 0.0956\n",
            "Binary Classification Test Accuracy: 91.64%\n"
          ]
        }
      ],
      "source": [
        "vector_size = 300\n",
        "output_channels1 = 50\n",
        "output_channels2 = 10\n",
        "output_size = 2\n",
        "\n",
        "model_binary, accuracy_binary, training_losses_binary = CNN_Model(\n",
        "    word2vec_binary_x_train, word2vec_binary_y_train, word2vec_binary_x_test, word2vec_binary_y_test, vector_size, output_channels1, output_channels2, output_size, own_word2vec_model.wv,\n",
        "    \"Word2Vec-custom\", \"CNN - (50 concat)\", \"Binary\", result\n",
        ")\n",
        "print(f'Binary Classification Test Accuracy: {accuracy_binary * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Below segment utilizes the CNN_Model function to train a Convolutional Neural Network (CNN) model for binary classification:\n",
        "\n",
        "- Model Configuration: The CNN model is configured with a word embedding size of 300 dimensions (vector_size). It consists of two convolutional layers with 50 and 10 output channels (output_channels1 and output_channels2, respectively), followed by a fully connected layer with an output size of 2 for binary classification (output_size).\n",
        "\n",
        "- Training: The CNN_Model function is called with training and testing data (word2vec_binary_x_train, word2vec_binary_y_train, word2vec_binary_x_test, word2vec_binary_y_test) and the specified model configuration. It trains the CNN model using the provided Word2Vec embeddings (pretrained_word2vec_model), monitors the training process, and computes the testing accuracy on the test data.\n",
        "\n",
        "- Output: After training, the code prints the test accuracy for binary classification, indicating how accurately the CNN model can predict binary sentiment labels based on the provided pre-trained Word2Vec embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Training (CNN - (50 concat)) - (Word2Vec-Pretrained) - (Binary) : \n",
            "\n",
            "Model configuration :\n",
            "Sequential(\n",
            "  (0): Conv1d(300, 50, kernel_size=(3,), stride=(1,))\n",
            "  (1): ReLU()\n",
            "  (2): Conv1d(50, 10, kernel_size=(3,), stride=(1,))\n",
            "  (3): ReLU()\n",
            "  (4): Flatten(start_dim=1, end_dim=-1)\n",
            "  (5): Linear(in_features=460, out_features=2, bias=True)\n",
            ")\n",
            "\n",
            "Training Loss for each epoch :\n",
            "Epoch [1/10], Loss: 0.2444\n",
            "Epoch [2/10], Loss: 0.1913\n",
            "Epoch [3/10], Loss: 0.1722\n",
            "Epoch [4/10], Loss: 0.1570\n",
            "Epoch [5/10], Loss: 0.1433\n",
            "Epoch [6/10], Loss: 0.1306\n",
            "Epoch [7/10], Loss: 0.1200\n",
            "Epoch [8/10], Loss: 0.1102\n",
            "Epoch [9/10], Loss: 0.1016\n",
            "Epoch [10/10], Loss: 0.0933\n",
            "Binary Classification Test Accuracy: 91.35%\n"
          ]
        }
      ],
      "source": [
        "vector_size = 300\n",
        "output_channels1 = 50\n",
        "output_channels2 = 10\n",
        "output_size = 2\n",
        "\n",
        "model_binary, accuracy_binary, training_losses_binary = CNN_Model(\n",
        "    word2vec_binary_x_train, word2vec_binary_y_train, word2vec_binary_x_test, word2vec_binary_y_test, vector_size, output_channels1, output_channels2, output_size, pretrained_word2vec_model,\n",
        "    \"Word2Vec-Pretrained\", \"CNN - (50 concat)\", \"Binary\", result\n",
        ")\n",
        "print(f'Binary Classification Test Accuracy: {accuracy_binary * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Below code segment employs the CNN_Model function to train a Convolutional Neural Network (CNN) model for ternary classification:\n",
        "\n",
        "- Model Configuration: The CNN model is configured with a word embedding size of 300 dimensions (vector_size). It comprises two convolutional layers with 50 and 10 output channels (output_channels1 and output_channels2, respectively), followed by a fully connected layer with an output size of 3 for ternary classification (output_size).\n",
        "\n",
        "- Training: The CNN_Model function is invoked with training and testing data (word2vec_ternary_x_train, word2vec_ternary_y_train, word2vec_ternary_x_test, word2vec_ternary_y_test) and the specified model configuration. The CNN model is trained utilizing the provided pre-trained Word2Vec embeddings (pretrained_word2vec_model). It monitors the training process and computes the testing accuracy on the test data.\n",
        "\n",
        "- Output: Upon completion of training, the code prints the test accuracy for ternary classification. This indicates how accurately the CNN model can predict ternary sentiment labels based on the provided pre-trained Word2Vec embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Training (CNN - (50 concat)) - (Word2Vec-Pretrained) - (Ternary) : \n",
            "\n",
            "Model configuration :\n",
            "Sequential(\n",
            "  (0): Conv1d(300, 50, kernel_size=(3,), stride=(1,))\n",
            "  (1): ReLU()\n",
            "  (2): Conv1d(50, 10, kernel_size=(3,), stride=(1,))\n",
            "  (3): ReLU()\n",
            "  (4): Flatten(start_dim=1, end_dim=-1)\n",
            "  (5): Linear(in_features=460, out_features=3, bias=True)\n",
            ")\n",
            "\n",
            "Training Loss for each epoch :\n",
            "Epoch [1/10], Loss: 0.5784\n",
            "Epoch [2/10], Loss: 0.5058\n",
            "Epoch [3/10], Loss: 0.4847\n",
            "Epoch [4/10], Loss: 0.4700\n",
            "Epoch [5/10], Loss: 0.4568\n",
            "Epoch [6/10], Loss: 0.4453\n",
            "Epoch [7/10], Loss: 0.4357\n",
            "Epoch [8/10], Loss: 0.4266\n",
            "Epoch [9/10], Loss: 0.4182\n",
            "Epoch [10/10], Loss: 0.4111\n",
            "Ternary Classification Test Accuracy: 77.18%\n"
          ]
        }
      ],
      "source": [
        "vector_size = 300\n",
        "output_channels1 = 50\n",
        "output_channels2 = 10\n",
        "output_size = 3\n",
        "\n",
        "model_ternary, accuracy_ternary, training_losses_ternary = CNN_Model(\n",
        "    word2vec_ternary_x_train, word2vec_ternary_y_train, word2vec_ternary_x_test, word2vec_ternary_y_test, vector_size, output_channels1, output_channels2, output_size,  pretrained_word2vec_model,\n",
        "    \"Word2Vec-Pretrained\", \"CNN - (50 concat)\", \"Ternary\", result\n",
        ")\n",
        "print(f'Ternary Classification Test Accuracy: {accuracy_ternary * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### This code segment employs the CNN_Model function to train a Convolutional Neural Network (CNN) model for ternary classification using custom Word2Vec embeddings. Here's a brief explanation:\n",
        "\n",
        "- Model Configuration: The CNN model is configured with a word embedding size of 300 dimensions (vector_size). It consists of two convolutional layers with 50 and 10 output channels (output_channels1 and output_channels2, respectively), followed by a fully connected layer with an output size of 3 for ternary classification (output_size).\n",
        "\n",
        "- Training: The CNN_Model function is called with training and testing data (word2vec_ternary_x_train, word2vec_ternary_y_train, word2vec_ternary_x_test, word2vec_ternary_y_test) and the specified model configuration. The CNN model is trained using the provided custom Word2Vec embeddings (own_word2vec_model.wv). It monitors the training process and computes the testing accuracy on the test data.\n",
        "\n",
        "- Output: After training, the code prints the test accuracy for ternary classification. This represents how accurately the CNN model can predict ternary sentiment labels based on the provided custom Word2Vec embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Training (CNN - (50 concat)) - (Word2Vec-Custom) - (Ternary) : \n",
            "\n",
            "Model configuration :\n",
            "Sequential(\n",
            "  (0): Conv1d(300, 50, kernel_size=(3,), stride=(1,))\n",
            "  (1): ReLU()\n",
            "  (2): Conv1d(50, 10, kernel_size=(3,), stride=(1,))\n",
            "  (3): ReLU()\n",
            "  (4): Flatten(start_dim=1, end_dim=-1)\n",
            "  (5): Linear(in_features=460, out_features=3, bias=True)\n",
            ")\n",
            "\n",
            "Training Loss for each epoch :\n",
            "Epoch [1/10], Loss: 0.5662\n",
            "Epoch [2/10], Loss: 0.5003\n",
            "Epoch [3/10], Loss: 0.4817\n",
            "Epoch [4/10], Loss: 0.4665\n",
            "Epoch [5/10], Loss: 0.4536\n",
            "Epoch [6/10], Loss: 0.4420\n",
            "Epoch [7/10], Loss: 0.4317\n",
            "Epoch [8/10], Loss: 0.4223\n",
            "Epoch [9/10], Loss: 0.4136\n",
            "Epoch [10/10], Loss: 0.4051\n",
            "Ternary Classification Test Accuracy: 78.28%\n"
          ]
        }
      ],
      "source": [
        "vector_size = 300\n",
        "output_channels1 = 50\n",
        "output_channels2 = 10\n",
        "output_size = 3\n",
        "\n",
        "model_ternary, accuracy_ternary, training_losses_ternary = CNN_Model(\n",
        "    word2vec_ternary_x_train, word2vec_ternary_y_train, word2vec_ternary_x_test, word2vec_ternary_y_test, vector_size, output_channels1, output_channels2, output_size,  own_word2vec_model.wv,\n",
        "    \"Word2Vec-Custom\", \"CNN - (50 concat)\", \"Ternary\", result\n",
        ")\n",
        "print(f'Ternary Classification Test Accuracy: {accuracy_ternary * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Results and Observation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Input Data</th>\n",
              "      <th>Model Name</th>\n",
              "      <th>Classification</th>\n",
              "      <th>Training Accuracy</th>\n",
              "      <th>Testing Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>TF-IDF</td>\n",
              "      <td>SVM</td>\n",
              "      <td>Binary</td>\n",
              "      <td>0.920781</td>\n",
              "      <td>0.908550</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Word2Vec-custom</td>\n",
              "      <td>SVM</td>\n",
              "      <td>Binary</td>\n",
              "      <td>0.882437</td>\n",
              "      <td>0.880775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Word2Vec-Pre-Trained</td>\n",
              "      <td>SVM</td>\n",
              "      <td>Binary</td>\n",
              "      <td>0.862769</td>\n",
              "      <td>0.858525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>TF-IDF</td>\n",
              "      <td>Perceptron</td>\n",
              "      <td>Binary</td>\n",
              "      <td>0.874138</td>\n",
              "      <td>0.870700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Word2Vec-custom</td>\n",
              "      <td>Perceptron</td>\n",
              "      <td>Binary</td>\n",
              "      <td>0.850669</td>\n",
              "      <td>0.850100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Word2Vec-Pre-Trained</td>\n",
              "      <td>Perceptron</td>\n",
              "      <td>Binary</td>\n",
              "      <td>0.838494</td>\n",
              "      <td>0.838725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>TF-IDF</td>\n",
              "      <td>Naive Bayes</td>\n",
              "      <td>Binary</td>\n",
              "      <td>0.869219</td>\n",
              "      <td>0.865400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>TF-IDF</td>\n",
              "      <td>Logistic</td>\n",
              "      <td>Binary</td>\n",
              "      <td>0.916825</td>\n",
              "      <td>0.907975</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Word2Vec-custom</td>\n",
              "      <td>Logistic</td>\n",
              "      <td>Binary</td>\n",
              "      <td>0.881644</td>\n",
              "      <td>0.880675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Word2Vec-Pre-Trained</td>\n",
              "      <td>Logistic</td>\n",
              "      <td>Binary</td>\n",
              "      <td>0.861375</td>\n",
              "      <td>0.857400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Word2Vec-Custom</td>\n",
              "      <td>FNN - (Concat)</td>\n",
              "      <td>Ternary</td>\n",
              "      <td>-</td>\n",
              "      <td>0.696780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Word2Vec-Pretrained</td>\n",
              "      <td>FNN - (Concat)</td>\n",
              "      <td>Ternary</td>\n",
              "      <td>-</td>\n",
              "      <td>0.686700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Word2Vec-custom</td>\n",
              "      <td>FNN - (Concat)</td>\n",
              "      <td>Binary</td>\n",
              "      <td>-</td>\n",
              "      <td>0.857075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Word2Vec-Pretrained</td>\n",
              "      <td>FNN - (Concat)</td>\n",
              "      <td>Binary</td>\n",
              "      <td>-</td>\n",
              "      <td>0.849725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Word2Vec-Custom</td>\n",
              "      <td>FNN - (Average)</td>\n",
              "      <td>Ternary</td>\n",
              "      <td>-</td>\n",
              "      <td>0.762120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Word2Vec-Pretrained</td>\n",
              "      <td>FNN - (Average)</td>\n",
              "      <td>Ternary</td>\n",
              "      <td>-</td>\n",
              "      <td>0.742860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Word2Vec-custom</td>\n",
              "      <td>FNN - (Average)</td>\n",
              "      <td>Binary</td>\n",
              "      <td>-</td>\n",
              "      <td>0.899075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Word2Vec-Pretrained</td>\n",
              "      <td>FNN - (Average)</td>\n",
              "      <td>Binary</td>\n",
              "      <td>-</td>\n",
              "      <td>0.887600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Word2Vec-Pretrained</td>\n",
              "      <td>CNN - (50 concat)</td>\n",
              "      <td>Ternary</td>\n",
              "      <td>-</td>\n",
              "      <td>0.771840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Word2Vec-Custom</td>\n",
              "      <td>CNN - (50 concat)</td>\n",
              "      <td>Ternary</td>\n",
              "      <td>-</td>\n",
              "      <td>0.782760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Word2Vec-custom</td>\n",
              "      <td>CNN - (50 concat)</td>\n",
              "      <td>Binary</td>\n",
              "      <td>-</td>\n",
              "      <td>0.916350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Word2Vec-Pretrained</td>\n",
              "      <td>CNN - (50 concat)</td>\n",
              "      <td>Binary</td>\n",
              "      <td>-</td>\n",
              "      <td>0.913500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Input Data         Model Name Classification Training Accuracy  \\\n",
              "0                 TF-IDF                SVM         Binary          0.920781   \n",
              "1        Word2Vec-custom                SVM         Binary          0.882437   \n",
              "2   Word2Vec-Pre-Trained                SVM         Binary          0.862769   \n",
              "3                 TF-IDF         Perceptron         Binary          0.874138   \n",
              "4        Word2Vec-custom         Perceptron         Binary          0.850669   \n",
              "5   Word2Vec-Pre-Trained         Perceptron         Binary          0.838494   \n",
              "6                 TF-IDF        Naive Bayes         Binary          0.869219   \n",
              "7                 TF-IDF           Logistic         Binary          0.916825   \n",
              "8        Word2Vec-custom           Logistic         Binary          0.881644   \n",
              "9   Word2Vec-Pre-Trained           Logistic         Binary          0.861375   \n",
              "10       Word2Vec-Custom     FNN - (Concat)        Ternary                 -   \n",
              "11   Word2Vec-Pretrained     FNN - (Concat)        Ternary                 -   \n",
              "12       Word2Vec-custom     FNN - (Concat)         Binary                 -   \n",
              "13   Word2Vec-Pretrained     FNN - (Concat)         Binary                 -   \n",
              "14       Word2Vec-Custom    FNN - (Average)        Ternary                 -   \n",
              "15   Word2Vec-Pretrained    FNN - (Average)        Ternary                 -   \n",
              "16       Word2Vec-custom    FNN - (Average)         Binary                 -   \n",
              "17   Word2Vec-Pretrained    FNN - (Average)         Binary                 -   \n",
              "18   Word2Vec-Pretrained  CNN - (50 concat)        Ternary                 -   \n",
              "19       Word2Vec-Custom  CNN - (50 concat)        Ternary                 -   \n",
              "20       Word2Vec-custom  CNN - (50 concat)         Binary                 -   \n",
              "21   Word2Vec-Pretrained  CNN - (50 concat)         Binary                 -   \n",
              "\n",
              "    Testing Accuracy  \n",
              "0           0.908550  \n",
              "1           0.880775  \n",
              "2           0.858525  \n",
              "3           0.870700  \n",
              "4           0.850100  \n",
              "5           0.838725  \n",
              "6           0.865400  \n",
              "7           0.907975  \n",
              "8           0.880675  \n",
              "9           0.857400  \n",
              "10          0.696780  \n",
              "11          0.686700  \n",
              "12          0.857075  \n",
              "13          0.849725  \n",
              "14          0.762120  \n",
              "15          0.742860  \n",
              "16          0.899075  \n",
              "17          0.887600  \n",
              "18          0.771840  \n",
              "19          0.782760  \n",
              "20          0.916350  \n",
              "21          0.913500  "
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results_df = pd.DataFrame(result, columns=result_column).sort_values(['Model Name', 'Classification'], ascending=False, ignore_index=True)\n",
        "results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Terminologies\n",
        "- \"Custom\" -> Word2vec model trained on given data set\n",
        "- \"Pretrained\" -> Word2vec model trained by \n",
        "- \"Concat\" -> concatenate the first 10 Word2Vec vectors for each review as the input feature \n",
        "- \"50 concat\" -> limit the maximum review length to 50 by truncating longer reviews and padding shorter reviews with a null value \n",
        "- \"Average\" -> average Word2Vec vectors for each review as the input feature [(x = 1/N summation (i = 1 to i = N) for Wi ] for a review with N words.\n",
        "\n",
        "\n",
        "### Observations\n",
        "- Comparison of Semantic Similarities and Encoding Semantic Similarities:\n",
        "\n",
        "    - The pretrained Word2Vec model tends to produce more intuitive semantic similarities between words compared to the Word2Vec model trained on your own dataset.\n",
        "\n",
        "    - For example, in the comparison of \"summer - hot + cold = winter\", the pretrained model produces a semantic similarity with \"winter\", which aligns well with our expectations. However, the similarity score is relatively lower in the model trained on your own dataset.\n",
        "\n",
        "    - Similarly, in the comparison of \"worse - bad + good = better\", the pretrained model again produces a semantic similarity with \"better\", whereas the model trained on your dataset returns \"excellent\" with a lower similarity score.\n",
        "\n",
        "    - The same trend is observed in the comparison of \"dad - mom + daughter = son\", where the pretrained model provides a more relevant semantic similarity.\n",
        "\n",
        "- Encoding Semantic Similarities:\n",
        "\n",
        "    - The pretrained Word2Vec model, trained on a large corpus of diverse texts, seems to encode semantic similarities between words better. This is likely because the pretrained model has been trained on a vast amount of data, capturing richer semantic relationships.\n",
        "\n",
        "    - On the other hand, the Word2Vec model trained on your own dataset may not have been exposed to as much diverse linguistic context, leading to less accurate semantic embeddings.\n",
        "\n",
        "<ins> In summary, the pretrained Word2Vec model generally performs better in encoding semantic similarities between words, likely due to its training on a larger and more diverse dataset. However, the model trained on your own dataset can still be valuable if it captures domain-specific nuances or vocabulary not present in the pretrained model.</ins>\n",
        "\n",
        "\n",
        "\n",
        "- Feature Type Performance Comparison :\n",
        "\n",
        "    - TF-IDF features showcase strong performance, especially notable in the SVM model for binary classification (Testing Accuracy: 0.908550), highlighting its effectiveness in distinguishing relevant textual features for POS tagging.\n",
        "    \n",
        "    - Your trained Word2Vec (custom Word2Vec)  embeddings outperform the pretrained variants across several models, indicating that embeddings tailored to the POS tagging task capture more relevant semantic nuances. This is particularly evident in the SVM model (Testing Accuracy: 0.880775) and the FNN model utilizing average Word2Vec embeddings for binary classification (Testing Accuracy: 0.899075).\n",
        "    \n",
        "    - Pretrained Word2Vec embeddings, while beneficial, generally underperform in comparison to custom embeddings and TF-IDF features, likely due to the generic nature of their training data.\n",
        "\n",
        "<ins>In summary, TF-IDF features perform well overall, while custom Word2Vec embeddings tailored to the dataset tend to yield the highest accuracy, outperforming both pre-trained Word2Vec and TF-IDF features.</ins>\n",
        "\n",
        "\n",
        "- Model Accuracy Comparison : \n",
        "    \n",
        "    - Incorporating character-level information through a CNN module with custom Word2Vec embeddings notably improves model performance in both binary and ternary classifications, with the CNN model achieving a Testing Accuracy of 0.916350 for binary classification..\n",
        "        \n",
        "    - FNN models, particularly with the \"Average\" strategy and custom Word2Vec embeddings, demonstrate substantial effectiveness, achieving a Testing Accuracy of 0.899075 for binary classification and 0.762120 for ternary classification, highlighting the strength of neural network approaches in POS tagging.\n",
        "        \n",
        "<ins> Overall, FNN models provide competitive accuracy values compared to SVM and Perceptron models, showcasing the potential of neural networks for text classification tasks.</ins>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
